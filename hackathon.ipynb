{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayered Neural Network, Keras API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Conv2D, MaxPooling2D, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_keypoints =  [ 'nose', 'left_eye', 'right_eye', 'left_ear', 'right_ear', 'left_shoulder', 'right_shoulder', \n",
    "                   'left_elbow', 'right_elbow', 'left_wrist', 'right_wrist', 'left_hip', 'right_hip', 'left_knee', \n",
    "                   'right_knee', 'left_ankle', 'right_ankle' ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset to pass into the neural network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_classifications = ['HoldingBall-45deg-r', 'CrossedArms-90deg-r', 'HeadShot', 'Hero',\n",
    "                        'HandsOnHips-45deg-r','FullBody', 'CrossedArms-frontal', 'Celebration', \n",
    "                        'HandsOnHips-90deg-l', 'HalfBody', 'HoldingBall', 'CrossedArms-90deg-l', \n",
    "                        'HoldingBall-45deg-l', 'CrossedArms-45deg-l', 'HandsOnHips-45deg-l', \n",
    "                        'HandsOnHips-90deg-r', 'CrossedArms-45deg-r'\n",
    "]\n",
    "\n",
    "num_classes = len(pose_classifications)         # Number of classifications\n",
    "num_images = 69\n",
    "\n",
    "# Tensors are the input data and output classification in a Neural Network\n",
    "\n",
    "# Covert to TensorFlow tensors - Different fields have different definitions \n",
    "input = [num_images, 52]                    # Placeholder till carl gets the MoveNet Data     \n",
    "output = [num_images, num_classes]          # Each class parameter will hold the probability of it being that class \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set low confidence values to 0\n",
    "- This means that These points have not showed up in the image\n",
    "  - Easier to classify between full-body, half-body and head shot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(input, output, test_size=0.2, random_state=69)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Multi layered Neural Network \n",
    "- keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Model\n",
    "model = Sequential()                # Input Layer -> Hidden Layers -> Output Layer \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization layer \n",
    "- Points between (0,1)\n",
    "- Confidence level is already between (0,1) since it is a probability "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalisation_layer = keras.layers.Normalization(axis=1)    # Preprocessing Layer\n",
    "normalisation_layer.adapt(input)                            # Fit layer to input data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Layers to the Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(normalisation_layer)\n",
    "# 2 hidden layers \n",
    "# 2/3 nodes heuristic\n",
    "\n",
    "# 52 input nodes for input layer \n",
    "\n",
    "# First hidden layer 42\n",
    "model.add(Dense(units=42, activation='relu', input_shape = (52,)))\n",
    "model.add(Dropout(0.2))       # Randomly drop neurons so not reliant on one \n",
    "\n",
    "# Second hidden layer 28\n",
    "model.add(Dense(units=28, activation='relu'))   # No need for input since it is the next level of the neural network \n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Third hidden layer 18\n",
    "model.add(Dense(units=18, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Output layer \n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Optimizer (which adjusts weights)\n",
    "# loss function (which calculates how far off the predictions are)\n",
    "# Metrics (which track the model's performance during training).\n",
    "\n",
    "# categorical_crossentropy since there are multiple classes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test,y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding new image data to predict\n",
    "\n",
    "\n",
    "predictions = model.predict(X_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model \n",
    "- Architecture\n",
    "- Weights\n",
    "- Training configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('AWSHack.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('AWSHack.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):        # hp - Hyperparameters\n",
    "  model = Sequential()\n",
    "  normalisation_layer = keras.layers.Normalization(axis=1)   \n",
    "  normalisation_layer.adapt(input)                           \n",
    "  model.add(normalisation_layer)\n",
    "  model.add(Dense(units=42, activation='relu', input_shape = (52,)))  \n",
    "  model.add(Dropout(0.2))      \n",
    "  model.add(Dense(units=28, activation='relu'))\n",
    "  model.add(Dropout(0.2))\n",
    "  model.add(Dense(units=18, activation='relu'))\n",
    "  model.add(Dropout(0.2))\n",
    "  model.add(Dense(num_classes, activation='softmax'))   # num_classes = 17\n",
    "  model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "# Initialise tuner\n",
    "tuner = keras_tuner.RandomSearch(build_model, objective='val_loss', max_trials = 5)\n",
    "\n",
    "# Search for the best model \n",
    "tuner.search(X_train, y_train, epochs=5, validation_data=(X_test, y_test))\n",
    "best_model = tuner.get_best_models()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the folders into categories !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naieve Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.layers import Input, Flatten, Dense \n",
    "from tensorflow.keras.models import Sequential, Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current working directory \n",
    "folder_0 = os.path.join(os.getcwd(), 'resized_images', '0')\n",
    "folder_1 = os.path.join(os.getcwd(), 'resized_images', '1')\n",
    "folder_2 = os.path.join(os.getcwd(), 'resized_images', '2')\n",
    "folder_3 = os.path.join(os.getcwd(), 'resized_images', '3')\n",
    "folder_4 = os.path.join(os.getcwd(), 'resized_images', '4')\n",
    "folder_5 = os.path.join(os.getcwd(), 'resized_images', '5')\n",
    "folder_6 = os.path.join(os.getcwd(), 'resized_images', '6')\n",
    "folder_7 = os.path.join(os.getcwd(), 'resized_images', '7')\n",
    "folder_8 = os.path.join(os.getcwd(), 'resized_images', '8')\n",
    "folder_9 = os.path.join(os.getcwd(), 'resized_images', '9')\n",
    "folder_10 = os.path.join(os.getcwd(), 'resized_images', '10')\n",
    "folder_11 = os.path.join(os.getcwd(), 'resized_images', '11')\n",
    "folder_12 = os.path.join(os.getcwd(), 'resized_images', '12')\n",
    "folder_13 = os.path.join(os.getcwd(), 'resized_images', '13')\n",
    "folder_14 = os.path.join(os.getcwd(), 'resized_images', '14')\n",
    "folder_15 = os.path.join(os.getcwd(), 'resized_images', '15')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 12 images from class 0\n",
      "Loading 12 images from class 1\n",
      "Loading 12 images from class 2\n",
      "Loading 12 images from class 3\n",
      "Loading 12 images from class 4\n",
      "Loading 12 images from class 5\n",
      "Loading 12 images from class 6\n",
      "Loading 12 images from class 7\n",
      "Loading 12 images from class 8\n",
      "Loading 12 images from class 9\n",
      "Loading 12 images from class 10\n",
      "Loading 12 images from class 11\n",
      "Loading 12 images from class 12\n",
      "Loading 12 images from class 13\n",
      "Loading 12 images from class 14\n",
      "Loading 12 images from class 15\n",
      "Loaded 192 images with shape (192, 2500, 1650, 3)\n",
      "Labels shape: (192,)\n"
     ]
    }
   ],
   "source": [
    "def prepare_image_data(base_path='resized_images', num_classes=16):\n",
    "    \"\"\"\n",
    "    Load images from folders 0-15 and prepare them with corresponding labels\n",
    "    for neural network training.\n",
    "    \n",
    "    Args:\n",
    "        base_path (str): Path to the directory containing class folders\n",
    "        num_classes (int): Number of class folders (0 to num_classes-1)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (images_array, labels_array) - NumPy arrays ready for neural network\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    # Loop through each class folder\n",
    "    for class_idx in range(num_classes):\n",
    "        folder_path = os.path.join(os.getcwd(), base_path, str(class_idx))\n",
    "        \n",
    "        # Check if the folder exists\n",
    "        if not os.path.exists(folder_path):\n",
    "            print(f\"Warning: Folder {folder_path} does not exist. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Get all image files in the folder\n",
    "        image_files = [f for f in os.listdir(folder_path) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        \n",
    "        print(f\"Loading {len(image_files)} images from class {class_idx}\")\n",
    "        \n",
    "        # Load each image\n",
    "        for img_file in image_files:\n",
    "            img_path = os.path.join(folder_path, img_file)\n",
    "            try:\n",
    "                # Read the image\n",
    "                img = cv2.imread(img_path)\n",
    "                \n",
    "                # Check if image was loaded successfully\n",
    "                if img is None:\n",
    "                    print(f\"Failed to load image: {img_path}\")\n",
    "                    continue\n",
    "                \n",
    "                # Add the image and its label to our lists\n",
    "                images.append(img)\n",
    "                labels.append(class_idx)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {img_path}: {e}\")\n",
    "    \n",
    "    # Convert lists to numpy arrays\n",
    "    X = np.array(images)\n",
    "    y = np.array(labels)\n",
    "    \n",
    "    print(f\"Loaded {len(images)} images with shape {X.shape}\")\n",
    "    print(f\"Labels shape: {y.shape}\")\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "\n",
    "image_data, label_data = prepare_image_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(192, 2500, 1650, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten the Input Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(image_data, label_data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Georg\\anaconda3\\envs\\steam_analysis\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "{{function_node __wrapped__StatelessRandomUniformV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} OOM when allocating tensor with shape[12375000,128] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu [Op:StatelessRandomUniformV2] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mResourceExhaustedError\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#model.add(Input(shape=(2500, 1650, 3)))\u001b[39;00m\n\u001b[32m      4\u001b[39m model.add(Flatten(input_shape=(\u001b[32m2500\u001b[39m,\u001b[32m1650\u001b[39m,\u001b[32m3\u001b[39m)))\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[43munits\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrelu\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m model.add(Dense(units=\u001b[32m256\u001b[39m, activation=\u001b[33m'\u001b[39m\u001b[33mrelu\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m      7\u001b[39m model.add(Dense(units=\u001b[32m512\u001b[39m, activation=\u001b[33m'\u001b[39m\u001b[33mrelu\u001b[39m\u001b[33m'\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Georg\\anaconda3\\envs\\steam_analysis\\Lib\\site-packages\\keras\\src\\models\\sequential.py:122\u001b[39m, in \u001b[36mSequential.add\u001b[39m\u001b[34m(self, layer, rebuild)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;28mself\u001b[39m._layers.append(layer)\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m rebuild:\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_rebuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28mself\u001b[39m.built = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Georg\\anaconda3\\envs\\steam_analysis\\Lib\\site-packages\\keras\\src\\models\\sequential.py:149\u001b[39m, in \u001b[36mSequential._maybe_rebuild\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m._layers[\u001b[32m0\u001b[39m], InputLayer) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._layers) > \u001b[32m1\u001b[39m:\n\u001b[32m    148\u001b[39m     input_shape = \u001b[38;5;28mself\u001b[39m._layers[\u001b[32m0\u001b[39m].batch_shape\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m._layers[\u001b[32m0\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33minput_shape\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._layers) > \u001b[32m1\u001b[39m:\n\u001b[32m    151\u001b[39m     \u001b[38;5;66;03m# We can build the Sequential model if the first layer has the\u001b[39;00m\n\u001b[32m    152\u001b[39m     \u001b[38;5;66;03m# `input_shape` property. This is most commonly found in Functional\u001b[39;00m\n\u001b[32m    153\u001b[39m     \u001b[38;5;66;03m# model.\u001b[39;00m\n\u001b[32m    154\u001b[39m     input_shape = \u001b[38;5;28mself\u001b[39m._layers[\u001b[32m0\u001b[39m].input_shape\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Georg\\anaconda3\\envs\\steam_analysis\\Lib\\site-packages\\keras\\src\\layers\\layer.py:229\u001b[39m, in \u001b[36mLayer.__new__.<locals>.build_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    227\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m obj._open_name_scope():\n\u001b[32m    228\u001b[39m     obj._path = current_path()\n\u001b[32m--> \u001b[39m\u001b[32m229\u001b[39m     \u001b[43moriginal_build_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[38;5;66;03m# Record build config.\u001b[39;00m\n\u001b[32m    231\u001b[39m signature = inspect.signature(original_build_method)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Georg\\anaconda3\\envs\\steam_analysis\\Lib\\site-packages\\keras\\src\\models\\sequential.py:195\u001b[39m, in \u001b[36mSequential.build\u001b[39m\u001b[34m(self, input_shape)\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._layers[\u001b[32m1\u001b[39m:]:\n\u001b[32m    194\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m195\u001b[39m         x = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[32m    197\u001b[39m         \u001b[38;5;66;03m# Can happen if shape inference is not implemented.\u001b[39;00m\n\u001b[32m    198\u001b[39m         \u001b[38;5;66;03m# TODO: consider reverting inbound nodes on layers processed.\u001b[39;00m\n\u001b[32m    199\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Georg\\anaconda3\\envs\\steam_analysis\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Georg\\anaconda3\\envs\\steam_analysis\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\random.py:34\u001b[39m, in \u001b[36muniform\u001b[39m\u001b[34m(shape, minval, maxval, dtype, seed)\u001b[39m\n\u001b[32m     32\u001b[39m dtype = dtype \u001b[38;5;129;01mor\u001b[39;00m floatx()\n\u001b[32m     33\u001b[39m seed = _cast_seed(draw_seed(seed))\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrandom\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstateless_uniform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mminval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mminval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaxval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaxval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mResourceExhaustedError\u001b[39m: {{function_node __wrapped__StatelessRandomUniformV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} OOM when allocating tensor with shape[12375000,128] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu [Op:StatelessRandomUniformV2] name: "
     ]
    }
   ],
   "source": [
    "# Input Shape \n",
    "model = Sequential()\n",
    "#model.add(Input(shape=(2500, 1650, 3)))\n",
    "model.add(Flatten(input_shape=(2500,1650)))\n",
    "model.add(Dense(units=128, activation='relu'))\n",
    "model.add(Dense(units=256, activation='relu'))\n",
    "model.add(Dense(units=512, activation='relu'))\n",
    "model.add(Dense(units=16, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=50)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "steam_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
